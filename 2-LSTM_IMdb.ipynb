{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otT8VuKvWSap"
      },
      "source": [
        "<center>\n",
        "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"150\">\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<h1>Curso Procesamiento de Lenguaje Natural</h1>\n",
        "\n",
        "<h3>LSTM con Keras, un flujo básico pero completo</h3>\n",
        "\n",
        "\n",
        "<p> Julio Waissman Vilanova </p>\n",
        "<p>\n",
        "<img src=\"https://identidadbuho.unison.mx/wp-content/uploads/2019/06/letragrama-cmyk-72.jpg\" width=\"150\">\n",
        "</p>\n",
        "\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/pln/blob/main/labs/RNN/LSTM-IMdb.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\"  width=\"30\" /> Ejecuta en Colab</a>\n",
        "\n",
        "<p>\n",
        "Tomado parcialmente y adaptado de varias libretas de la documentación de Keras\n",
        "</p>\n",
        "\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsdoiB42WSat"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OxfZeF9WSau"
      },
      "source": [
        "# Obteniendo datos\n",
        "\n",
        "Vamos a recuperar la base de datos globera de IMdb que se usa para probar casi todos los modelos. Vamos a recuperar los adatos de\n",
        "\n",
        "``https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3p0Zyq2WSav",
        "outputId": "282b8d49-e4aa-4ef9-8bfa-c549108b86cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  20.4M      0  0:00:03  0:00:03 --:--:-- 20.4M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7ae8Bx2WSav"
      },
      "source": [
        " y vamos a investigas la estructura y lo que hay..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7gJkW8aWSav",
        "outputId": "c0fb019d-5873-4d38-fba4-ed45c2a1ddfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIYNK9emWSaw",
        "outputId": "dddcfdb6-5cf9-4574-cac6-699da6441aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R_hbVREWSaw",
        "outputId": "a2c20a5a-aa65-4dbb-8f0a-362b8f778dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  unsup  unsupBow.feat  urls_neg.txt  urls_pos.txt  urls_unsup.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVYqeTv6WSaw",
        "outputId": "db716641-d2b4-4be5-8272-cfd2f505b5f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeeUEr02WSaw"
      },
      "source": [
        "Solo nos interesan las evaluaciones positivas y negativas (para hacer una simple clasificación binaria y simplificar la aplicación), por lo que vamos a borrar el folder `unsup`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYTIUr5MWSax"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2MokEuPWSax"
      },
      "source": [
        "Ahora si, vamos a usar las librerías de `Keras` para leer los datos usando [`keras.utils.text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory).\n",
        "\n",
        "En este momento es donde tenemos que determinar el tamaño de los lotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccL-YcX0WSax",
        "outputId": "c3b79fed-ee5f-4839-f085-3c6815437cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32         # Tamaño de los minibatches\n",
        "\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp2BLcOTWSax",
        "outputId": "4083651a-fe4a-4471-f725-c060210c7903",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de batches en raw_train_ds: 625\n",
            "Numero de batches en raw_val_ds: 157\n",
            "Numero de batches en raw_test_ds: 782\n"
          ]
        }
      ],
      "source": [
        "print(f\"Numero de batches en raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Numero de batches en raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Numero de batches en raw_test_ds: {raw_test_ds.cardinality()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWq0uXHPWSax"
      },
      "source": [
        "Es importante revisar los datos crudos para tener una idea de como se recuperaron y cual es la forma que tienen.\n",
        "\n",
        "Esto lo podemos hacer tomando algunos datos de cada batch e imprimiendolos:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(textwrap.fill(text_batch.numpy()[i], 80, subsequent_indent='> '))\n",
        "        print(\"\\ntarget =\", label_batch.numpy()[i])"
      ],
      "metadata": {
        "id": "8OwZrRtUbGMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSiDGdAvWSay"
      },
      "source": [
        "## Preparando los datos\n",
        "\n",
        "Vamos ahora a convertir cada string de datos en una serie de índices numéricos, los cuales puedan entrar en\n",
        "un modelo neuronal. Para esto, vamos a generar índices a partir de las palabras existentesd en el texto.\n",
        "\n",
        "Este métdo puede ser no el mejor, ya que el vocabulario se fija en relación al vocabulario encontrado en el\n",
        "conjunto de aprendizaje. Más adelante veremos mñetodos más sofisticados para hacer la indezación, o como\n",
        "usar un vocabulario indexado ya preestablecido.\n",
        "\n",
        "Por el momento vamos primero a especificar el proceso de limpieza de texto (preprocesamiento) el cual será muy sencillo para este ejemplo y consiste en:\n",
        "\n",
        "1. Convertir a minúsculas todas las letras\n",
        "2. Eliminar los saltos de linea en formato *html* ( `<br /> `)\n",
        "3. Eliminar los signos de puntuación\n",
        "\n",
        "Igualmente, vamos a generar los minibatches con secuencias de `sequence_length` palabras. Esto es, si es insuficiente, se trunca el texto y si es\n",
        "demasiado, se completa el texto con 0's. De esa manera, todos los modelos aprenden con secuencias del mismo tamaño.\n",
        "\n",
        "Se utilizan hasta `max_features` tokens diferentes. De haber más, estos se eliminan en función de su frecuencia.\n",
        "\n",
        "Para esto vamos a utilizar la capa de `Keras` de [`layers.TextVectorization`](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvarU3LPWSay"
      },
      "outputs": [],
      "source": [
        "# Model constants.\n",
        "max_features = 20000\n",
        "sequence_length = 500\n",
        "\n",
        "# Preprocesamiento\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Capa de vectorización (encontrar los índices por palabra)\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAXem8EiWSay"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYNyygnsWSay"
      },
      "outputs": [],
      "source": [
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnhIB5ZsWSay"
      },
      "outputs": [],
      "source": [
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng0RE3EIWSay",
        "outputId": "eea1ce2e-2bf0-4ef9-ebb3-98a90f696e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Donde se guardan los datos de entrenamiento\n",
            "train_ds.cardinality() =  tf.Tensor(625, shape=(), dtype=int64)\n",
            "\n",
            "Y un minibatch se representa de esta manera: \n",
            "\n",
            "(<tf.Tensor: shape=(32, 500), dtype=int64, numpy=\n",
            "array([[  10,    7,  240, ...,    0,    0,    0],\n",
            "       [  10,   17,   13, ...,    0,    0,    0],\n",
            "       [  89,   76,   69, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [1196,   50,   11, ...,    0,    0,    0],\n",
            "       [  47,   13,  136, ...,    0,    0,    0],\n",
            "       [  11,  727,  159, ...,    0,    0,    0]])>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
            "array([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Donde se guardan los datos de entrenamiento\")\n",
        "print(\"train_ds.cardinality() = \", train_ds.cardinality())\n",
        "\n",
        "ejemplo = train_ds.take(1)\n",
        "\n",
        "print(\"\\nY un minibatch se representa de esta manera: \\n\")\n",
        "print(ejemplo.get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIFSVaOTWSay"
      },
      "source": [
        "## Modelo basado en LSTM multicapa\n",
        "\n",
        "Vamos a hacer un modelo multicapa, el cual seguramente requerirá de ajustes de su parte.\n",
        "\n",
        "Vamos a utilizar la forma funcional de definir un modelo neuronal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCoXEe-zWSaz",
        "outputId": "af2492ac-9399-4bab-88b1-3c462e923704",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2823297 (10.77 MB)\n",
            "Trainable params: 2823297 (10.77 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "emb = 128               # Embedding size\n",
        "unidades = 128          # Hidden units per layer\n",
        "\n",
        "\n",
        "# Entrada en indices\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Capa de embeddings\n",
        "x = layers.Embedding(max_features, emb)(inputs)\n",
        "\n",
        "# Dos capas de LSTMs\n",
        "x = layers.LSTM(unidades, return_sequences=True)(x)\n",
        "x = layers.LSTM(unidades)(x)\n",
        "\n",
        "# Salida\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs, predictions)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlVxffPEWSaz"
      },
      "source": [
        "Compilamos y ponemos a aprender el modelo (usando BPTT en forma automñatica)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9XoNTm7WSaz",
        "outputId": "ac5b054e-95ad-427f-bbb6-710b8d7ae809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "625/625 [==============================] - 73s 108ms/step - loss: 0.6931 - accuracy: 0.5070 - val_loss: 0.6927 - val_accuracy: 0.5046\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 0.6856 - accuracy: 0.5224 - val_loss: 0.7069 - val_accuracy: 0.5038\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d89f83a1d80>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.compile(\n",
        "    \"adam\",\n",
        "    \"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jKsSvgTWSaz"
      },
      "source": [
        "Y probamos con los datos de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksl2GvaJWSaz",
        "outputId": "3af03bc5-2fb5-4775-d078-8c8a1e8ec36e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 13s 17ms/step - loss: 0.7034 - accuracy: 0.5027\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7033817172050476, 0.5027199983596802]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hphV3v9NWSaz"
      },
      "source": [
        "Y ahora vamos a probar con bi-LSTM, haciendo un poco más complicado (aunque no mucho) el código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ows5_pJWSa0",
        "outputId": "fa7d8df1-6661-43e5-d246-85e84ecba2e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, None, 256)         263168    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 256)               394240    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               32896     \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3250433 (12.40 MB)\n",
            "Trainable params: 3250433 (12.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "emb = 128\n",
        "unidades = 128\n",
        "\n",
        "# Input\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "# Capa de embeddings\n",
        "x = layers.Embedding(max_features, emb)(inputs)\n",
        "\n",
        "# bi-LSTMs\n",
        "x = layers.Bidirectional(\n",
        "    layers.LSTM(unidades, return_sequences=True)\n",
        ")(x)\n",
        "x = layers.Bidirectional(\n",
        "    layers.LSTM(unidades)\n",
        ")(x)\n",
        "\n",
        "# Vanilla hidden layer:\n",
        "x = layers.Dense(unidades, activation=\"relu\")(x)\n",
        "\n",
        "# Salida\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model_bi = keras.Model(inputs, predictions)\n",
        "model_bi.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rUxHNmTWSa0",
        "outputId": "4d64002e-d8d8-41d2-881f-c583861fd366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "625/625 [==============================] - 91s 136ms/step - loss: 0.5395 - accuracy: 0.7321 - val_loss: 0.5614 - val_accuracy: 0.7314\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 47s 76ms/step - loss: 0.5616 - accuracy: 0.7138 - val_loss: 0.8042 - val_accuracy: 0.6122\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d89e01f9c30>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model_bi.compile(\n",
        "    \"adam\",\n",
        "    \"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model_bi.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qrgkEmaWSa1",
        "outputId": "ee16864b-b410-42f4-ae99-fc9df6f1c82c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 21s 27ms/step - loss: 0.8747 - accuracy: 0.5814\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8746628165245056, 0.5814399719238281]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model_bi.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuWQ6ZvFWSa1"
      },
      "source": [
        "## Modelo por convolucionales de 1 dimensión\n",
        "\n",
        "Este modelo viene como modelo de base en Keras, y es un buen inicio para ver como usar convolucionales como modelos para PLN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwhBRk7vWSa1"
      },
      "outputs": [],
      "source": [
        "emb = 128\n",
        "unidades = 128\n",
        "ventana = 7\n",
        "drop= 0.5\n",
        "\n",
        "# Entrada\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Capa de embeddings\n",
        "x = layers.Embedding(max_features, emb)(inputs)\n",
        "x = layers.Dropout(drop)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(\n",
        "    unidades,\n",
        "    ventana,\n",
        "    padding=\"valid\",\n",
        "    activation=\"relu\",\n",
        "    strides=3\n",
        ")(x)\n",
        "x = layers.Conv1D(\n",
        "    unidades,\n",
        "    ventana,\n",
        "    padding=\"valid\",\n",
        "    activation=\"relu\",\n",
        "    strides=3\n",
        ")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# Vanilla hidden layer:\n",
        "x = layers.Dense(unidades, activation=\"relu\")(x)\n",
        "x = layers.Dropout(drop)(x)\n",
        "\n",
        "# Salida\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model_conv1d = tf.keras.Model(inputs, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZOl1yCcWSa1",
        "outputId": "4d6096e8-6329-486a-b781-393197f72810",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6572 - accuracy: 0.5377 - val_loss: 0.7320 - val_accuracy: 0.5012\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.6491 - accuracy: 0.5470 - val_loss: 0.7111 - val_accuracy: 0.5064\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.6462 - accuracy: 0.5401 - val_loss: 0.7445 - val_accuracy: 0.5040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d898215e890>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model_conv1d.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBMk-QzzWSa1",
        "outputId": "c2f92f89-ff91-4f4a-ca18-568d2fc2e616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 3ms/step - loss: 0.6931 - accuracy: 0.5048\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6931184530258179, 0.504800021648407]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model_conv1d.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyK8BQ11WSa1"
      },
      "source": [
        "## Modelo para producción\n",
        "\n",
        "Si ya tenemos nuestro modelo funcionando, y nos gusta, y queremos dejarlo en un formato que permita aplicarlo a los datos en crudo, es necesario empaquetar todo nuestro procedimiento en un solo procedimiento de principio a fin.\n",
        "\n",
        "Agregamos aqui el truco para empaqetar todo, cuando ya no se espera reentrenar el modelo (al menos no en el corto plazo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO4yFPg4WSa1"
      },
      "outputs": [],
      "source": [
        "modelo_seleccionado = model_bi\n",
        "\n",
        "# A string input\n",
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "\n",
        "# Turn vocab indices into predictions\n",
        "outputs = modelo_seleccionado(indices)\n",
        "\n",
        "# Our end to end model\n",
        "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "end_to_end_model.save('enredos.keras')\n",
        "model.save(\"mayonesa.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_to_end_model = keras.saving.load_model(\"enredos.keras\")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ],
      "metadata": {
        "id": "2J9TU9l5bLid"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}